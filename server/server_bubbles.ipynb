{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d37fcbaa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# SECTION 1: IMPORTS & DEPENDENCIES\n",
                "# ==========================================\n",
                "\n",
                "# NOTE: If running in Colab, uncomment and run the line below first:\n",
                "!pip install fastapi uvicorn pyngrok deepgram-sdk groq supabase networkx sentence-transformers nest_asyncio python-multipart httpx qrcode livekit livekit-api python-dotenv livekit-agents==0.7.2 livekit-plugins-deepgram==0.5.1\n",
                "\n",
                "import os\n",
                "import json\n",
                "import asyncio\n",
                "import uvicorn\n",
                "import networkx as nx\n",
                "import qrcode\n",
                "import httpx\n",
                "import nest_asyncio\n",
                "import uuid\n",
                "from typing import List, Dict, Optional, Any\n",
                "from datetime import datetime\n",
                "\n",
                "# FastAPI & Server\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from fastapi.middleware.cors import CORSMiddleware\n",
                "from pydantic import BaseModel\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# AI & DB\n",
                "from groq import Groq\n",
                "from supabase import create_client, Client\n",
                "from supabase.lib.client_options import ClientOptions\n",
                "\n",
                "# FIX: Import torch explicitly BEFORE sentence_transformers\n",
                "import torch\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# LiveKit (VERSION 0.7.2 COMPATIBLE IMPORTS)\n",
                "from livekit import api\n",
                "from livekit.plugins import deepgram\n",
                "from livekit.agents import AutoSubscribe, JobContext, Worker, WorkerOptions, JobRequest\n",
                "\n",
                "# Fix for Colab/Jupyter event loops\n",
                "nest_asyncio.apply()\n",
                "\n",
                "\n",
                "# ==========================================\n",
                "# SECTION 2: KEYS & CONFIGURATION\n",
                "# ==========================================\n",
                "\n",
                "class Settings:\n",
                "    # --- API KEYS ---\n",
                "    # DEEPGRAM: Speech-to-Text\n",
                "    DEEPGRAM_KEY: str = os.getenv(\"DEEPGRAM_KEY\", \"\")\n",
                "\n",
                "    # LIVEKIT: Real-time Audio/Video\n",
                "    LIVEKIT_URL: str = os.getenv(\"LIVEKIT_URL\", \"\")\n",
                "    LIVEKIT_API_KEY: str = os.getenv(\"LIVEKIT_API_KEY\", \"\")\n",
                "    LIVEKIT_API_SECRET: str = os.getenv(\"LIVEKIT_API_SECRET\", \"\")\n",
                "\n",
                "    # GROQ: LLM Inference\n",
                "    GROQ_KEY: str = os.getenv(\"GROQ_API_KEY\", \"\")\n",
                "\n",
                "    # NGROK: Public Tunneling\n",
                "    NGROK_TOKEN: str = os.getenv(\"NGROK_TOKEN\", \"\")\n",
                "\n",
                "    # SUPABASE: Database & Vectors\n",
                "    SUPABASE_URL: str = os.getenv(\"SUPABASE_URL\", \"\")\n",
                "    SUPABASE_KEY: str = os.getenv(\"SUPABASE_KEY\", \"\")\n",
                "\n",
                "    # AI Models\n",
                "    EMBEDDING_MODEL: str = \"all-MiniLM-L6-v2\"\n",
                "\n",
                "    # LLM Models\n",
                "    CONSULTANT_MODEL: str = \"llama-3.3-70b-versatile\" # Detailed, accurate, slower\n",
                "    WINGMAN_MODEL: str = \"llama-3.1-8b-instant\" # Fast, low-latency for real-time\n",
                "\n",
                "    # Server Settings\n",
                "    HOST: str = \"0.0.0.0\"\n",
                "    PORT: int = 8000\n",
                "\n",
                "settings = Settings()\n",
                "\n",
                "# Set Env Vars for LiveKit SDK to pick up automatically\n",
                "os.environ['LIVEKIT_URL'] = settings.LIVEKIT_URL\n",
                "os.environ['LIVEKIT_API_KEY'] = settings.LIVEKIT_API_KEY\n",
                "os.environ['LIVEKIT_API_SECRET'] = settings.LIVEKIT_API_SECRET\n",
                "\n",
                "# Global storage for live sessions (mapping user_id/room_name to session_id)\n",
                "LIVE_SESSIONS: Dict[str, str] = {}\n",
                "\n",
                "\n",
                "# ==========================================\n",
                "# SECTION 3: MAIN SERVICES & LOGIC\n",
                "# ==========================================\n",
                "\n",
                "# --- 3A. Intelligence Services (Graph, Vector, Brain) ---\n",
                "\n",
                "class GraphService:\n",
                "    \"\"\"Manages specific Knowledge Graphs for EACH connected user.\"\"\"\n",
                "    def __init__(self):\n",
                "        self.active_graphs: Dict[str, nx.Graph] = {}\n",
                "        try:\n",
                "            options = ClientOptions(postgrest_client_timeout=10)\n",
                "            options.storage = None # Ensure storage attribute exists\n",
                "            options.httpx_client = None # Ensure httpx_client attribute exists\n",
                "            self.supabase: Client = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY, options=options)\n",
                "            print(\"âœ… Graph Service: DB Connected\")\n",
                "        except Exception as e:\n",
                "            print(f\"âš ï¸ Graph Service Error: {e}\")\n",
                "            self.supabase = None\n",
                "\n",
                "    def load_graph(self, user_id: str):\n",
                "        if not self.supabase: return\n",
                "        try:\n",
                "            # Note: Assuming user_id is the primary key and TEXT type in DB\n",
                "            response = self.supabase.table(\"knowledge_graphs\").select(\"graph_data\").eq(\"user_id\", user_id).execute()\n",
                "            if response.data and response.data[0]['graph_data']:\n",
                "                data = response.data[0]['graph_data']\n",
                "                self.active_graphs[user_id] = nx.node_link_graph(data)\n",
                "                print(f\"âœ… Graph Service: Loaded {len(self.active_graphs[user_id].nodes)} nodes for {user_id}\")\n",
                "            else:\n",
                "                self.active_graphs[user_id] = nx.Graph()\n",
                "                print(f\"ðŸ†• Graph Service: New empty graph created for {user_id}\")\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Graph Service Error loading graph for {user_id}: {e}\")\n",
                "            self.active_graphs[user_id] = nx.Graph()\n",
                "\n",
                "    def save_graph(self, user_id: str):\n",
                "        if user_id not in self.active_graphs or not self.supabase: return\n",
                "        try:\n",
                "            graph_json = nx.node_link_data(self.active_graphs[user_id])\n",
                "            data = {\n",
                "                \"user_id\": user_id,\n",
                "                \"graph_data\": graph_json,\n",
                "                \"updated_at\": datetime.now().isoformat()\n",
                "            }\n",
                "            # UPSERT handles creation or update\n",
                "            self.supabase.table(\"knowledge_graphs\").upsert(data).execute()\n",
                "            print(f\"âœ… Graph Service: Saved graph for {user_id}\")\n",
                "            # Clean up local graph after saving\n",
                "            del self.active_graphs[user_id]\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Graph Service Error saving graph for {user_id}: {e}\")\n",
                "\n",
                "    def find_context(self, user_id: str, text: str, top_k: int = 5) -> str:\n",
                "        \"\"\"Finds relevant facts from the in-memory graph.\"\"\"\n",
                "        if user_id not in self.active_graphs:\n",
                "            return \"No known graph facts.\"\n",
                "        G = self.active_graphs[user_id]\n",
                "        text_lower = text.lower()\n",
                "        facts = []\n",
                "        nodes_found = set()\n",
                "\n",
                "        # Simple keyword matching for graph nodes\n",
                "        for node in G.nodes():\n",
                "            if str(node).lower() in text_lower or text_lower in str(node).lower():\n",
                "                nodes_found.add(node)\n",
                "\n",
                "        # Collect facts related to the found nodes\n",
                "        for u, v, data in G.edges(data=True):\n",
                "            if u in nodes_found or v in nodes_found:\n",
                "                rel = data.get('relation', 'related to')\n",
                "                facts.append(f\"Fact: {u} {rel} {v}\")\n",
                "\n",
                "        context_str = \"\\n\".join(list(set(facts)))\n",
                "        return context_str if context_str else \"No known graph facts.\"\n",
                "\n",
                "    def update_local_graph(self, user_id: str, updates: List[dict]):\n",
                "        \"\"\"Updates the in-memory graph with new relationships.\"\"\"\n",
                "        if user_id not in self.active_graphs:\n",
                "            print(f\"âš ï¸ Graph Service: No active graph for {user_id} to update.\")\n",
                "            return\n",
                "        if updates:\n",
                "            print(f\"âž• Graph Service: Updating graph for {user_id} with {len(updates)} new relationships.\")\n",
                "        for u in updates:\n",
                "            source = u.get('source')\n",
                "            target = u.get('target')\n",
                "            relation = u.get('relation', 'related')\n",
                "            if source and target:\n",
                "                self.active_graphs[user_id].add_edge(source, target, relation=relation)\n",
                "                # print(f\"   Added edge: {source} - {relation} - {target}\")\n",
                "\n",
                "\n",
                "class VectorService:\n",
                "    \"\"\"Long-Term Memory (Supabase Vector Store)\"\"\"\n",
                "    def __init__(self):\n",
                "        print(\"ðŸ§  Vector Service: Loading Embedding Model (MiniLM)...\")\n",
                "        self.model = SentenceTransformer(settings.EMBEDDING_MODEL)\n",
                "        options = ClientOptions(postgrest_client_timeout=10)\n",
                "        options.storage = None # Ensure storage attribute exists\n",
                "        options.httpx_client = None # Ensure httpx_client attribute exists\n",
                "        self.supabase: Client = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY, options=options)\n",
                "        print(\"âœ… Vector Service: Embedding Model Loaded & DB Connected\")\n",
                "\n",
                "    def search_memory(self, user_id: str, query: str) -> str:\n",
                "        \"\"\"Searches long-term memory via vector similarity.\"\"\"\n",
                "        if not self.supabase: return \"No relevant past memories.\"\n",
                "        try:\n",
                "            # Search logic remains the same, assuming 'match_memory' RPC exists and takes 'p_user_id' (uuid)\n",
                "            vec = self.model.encode(query).tolist()\n",
                "            res = self.supabase.rpc(\"match_memory\", {\n",
                "                \"query_embedding\": vec,\n",
                "                \"match_threshold\": 0.5,\n",
                "                \"match_count\": 3,\n",
                "                \"p_user_id\": user_id # User ID from LiveKit is TEXT, ensure it matches DB UUID type\n",
                "            }).execute()\n",
                "            memories = [f\"Memory: {item['content']}\" for item in res.data if item['content']]\n",
                "            return \"\\n\".join(memories) if memories else \"No relevant past memories.\"\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Vector Service Error searching memory: {e}\")\n",
                "            return \"Error searching past memories.\"\n",
                "\n",
                "    async def save_memory(self, user_id: str, content: str):\n",
                "        \"\"\"Saves a piece of content to the user's long-term memory asynchronously.\"\"\"\n",
                "        if not self.supabase or not content.strip():\n",
                "            return\n",
                "\n",
                "        def encode_sync(text):\n",
                "            return self.model.encode(text.strip()).tolist()\n",
                "\n",
                "        try:\n",
                "            # Run synchronous encoding in a separate thread\n",
                "            vec = await asyncio.to_thread(encode_sync, content)\n",
                "\n",
                "            data = {\n",
                "                \"user_id\": user_id,\n",
                "                \"content\": content.strip(),\n",
                "                \"embedding\": vec,\n",
                "            }\n",
                "            self.supabase.table(\"memory\").insert(data).execute()\n",
                "            print(f\"ðŸ’¾ Vector Service: Saved new memory for {user_id}.\")\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Vector Service Error saving memory: {e}\")\n",
                "\n",
                "\n",
                "class SessionService:\n",
                "    \"\"\"Manages the creation and logging for Live Wingman sessions.\"\"\"\n",
                "    def __init__(self):\n",
                "        options = ClientOptions(postgrest_client_timeout=10)\n",
                "        options.storage = None # Ensure storage attribute exists\n",
                "        options.httpx_client = None # Ensure httpx_client attribute exists\n",
                "        self.supabase: Client = create_client(settings.SUPABASE_URL, settings.SUPABASE_KEY, options=options)\n",
                "\n",
                "    def create_session_record(self, user_id: str, title: str = \"New Conversation\", summary: str = None) -> str:\n",
                "        \"\"\"Creates a new session record in the DB and returns the ID.\"\"\"\n",
                "        if not self.supabase: return str(uuid.uuid4())\n",
                "        try:\n",
                "            data = {\"user_id\": user_id, \"title\": title, \"summary\": summary}\n",
                "            # Remove None values\n",
                "            data = {k: v for k, v in data.items() if v is not None}\n",
                "            \n",
                "            result = self.supabase.table(\"sessions\").insert(data).execute()\n",
                "            if result.data:\n",
                "                return result.data[0]['id']\n",
                "            return str(uuid.uuid4())\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Session Service Error creating session: {e}\")\n",
                "            return str(uuid.uuid4())\n",
                "\n",
                "    def log_batch_messages(self, session_id: str, logs: List[Dict[str, Any]]):\n",
                "        \"\"\"Logs a batch of messages to session_logs.\"\"\"\n",
                "        if not self.supabase or not logs: return\n",
                "        try:\n",
                "            # Transform logs to match schema: {session_id, role, content}\n",
                "            db_logs = []\n",
                "            for log in logs:\n",
                "                role = log.get('speaker', 'unknown').lower()\n",
                "                content = log.get('text', '')\n",
                "                if content:\n",
                "                    db_logs.append({\n",
                "                        \"session_id\": session_id,\n",
                "                        \"role\": role,\n",
                "                        \"content\": content\n",
                "                    })\n",
                "            \n",
                "            if db_logs:\n",
                "                self.supabase.table(\"session_logs\").insert(db_logs).execute()\n",
                "                print(f\"ðŸ“ Session Service: Logged {len(db_logs)} messages for session {session_id}.\")\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Session Service Error logging batch: {e}\")\n",
                "\n",
                "    def fetch_consultant_history(self, user_id: str, limit: int = 5) -> str:\n",
                "        \"\"\"Fetches the last N Q&A pairs from consultant_logs.\"\"\"\n",
                "        if not self.supabase: return \"No past consultant history.\"\n",
                "        try:\n",
                "            # Assuming user_id is the UUID from auth.users\n",
                "            res = self.supabase.table(\"consultant_logs\").select(\"question, answer\").eq(\"user_id\", user_id).order(\"created_at\", desc=True).limit(limit).execute()\n",
                "\n",
                "            history_lines = []\n",
                "            for item in reversed(res.data): # Reverse to show oldest first\n",
                "                history_lines.append(f\"Q: {item['question']}\")\n",
                "                history_lines.append(f\"A: {item['answer']}\")\n",
                "\n",
                "            history_str = \"\\n\".join(history_lines)\n",
                "            return history_str if history_str else \"No past consultant history.\"\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Session Service Error fetching consultant history: {e}\")\n",
                "            return \"Error fetching past consultant history.\"\n",
                "\n",
                "    def log_consultant_qa(self, user_id: str, question: str, answer: str):\n",
                "        \"\"\"Logs the Q&A pair for the consultant mode.\"\"\"\n",
                "        if not self.supabase: return\n",
                "        try:\n",
                "            self.supabase.table(\"consultant_logs\").insert({\n",
                "                \"user_id\": user_id,\n",
                "                \"question\": question,\n",
                "                \"answer\": answer\n",
                "            }).execute()\n",
                "            print(f\"ðŸ“ Session Service: Logged new consultant Q&A for {user_id}.\")\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Session Service Error logging consultant Q&A: {e}\")\n",
                "\n",
                "\n",
                "class BrainService:\n",
                "    \"\"\"The Intelligence Layer (Groq/Llama 3)\"\"\"\n",
                "    def __init__(self):\n",
                "        self.client = Groq(api_key=settings.GROQ_KEY)\n",
                "        print(\"ðŸ§  Brain Service: Groq Client Initialized\")\n",
                "\n",
                "    def get_wingman_advice(self, user_id: str, transcript: str, graph_context: str, vector_context: str) -> str:\n",
                "        \"\"\"Uses the fast 8B model for real-time, low-latency advice.\"\"\"\n",
                "        system_prompt = (\n",
                "            \"You are **Bubbles**, a strategic real-time Wingman AI. Your role is to assist the user \"\n",
                "            \"during live conversations by analyzing their transcript and offering precise, context-aware guidance.\\n\\n\"\n",
                "\n",
                "            \"==================== RULES ====================\\n\"\n",
                "            \"1. **Analyze the transcript deeply.** Identify intent, emotional tone, missing details, \"\n",
                "            \"and opportunities for better responses.\\n\\n\"\n",
                "\n",
                "            \"2. **Use the provided GRAPH CONTEXT (Facts) and MEMORY CONTEXT (History).**\\n\"\n",
                "            \"   - If the graph contains explicit facts, reference them directly in your advice.\\n\"\n",
                "            \"   - If memory provides relevant past behavior, preferences, or patterns, use them naturally.\\n\\n\"\n",
                "\n",
                "            \"3. **Always provide exactly THREE pieces of guidance:**\\n\"\n",
                "            \"   A. **Context-Based Advice:** Give specific, actionable advice that uses details from \"\n",
                "            \"      the Graph Context or Memory.\\n\"\n",
                "            \"   B. **Clarification Request:** If anything in the transcript is unclear or incomplete, \"\n",
                "            \"      ask a direct question to disambiguate.\\n\"\n",
                "            \"   C. **Apology & Confirmation Statement:** Provide a polite apology for any uncertainty \"\n",
                "            \"      and indicate that the user can confirm the missing details for a more accurate response.\\n\\n\"\n",
                "\n",
                "            \"==================== METADATA ====================\\n\"\n",
                "            f\"USER ID: {user_id}\\n\\n\"\n",
                "            f\"GRAPH CONTEXT (Facts):\\n{graph_context}\\n\\n\"\n",
                "            f\"MEMORY CONTEXT (History):\\n{vector_context}\\n\"\n",
                "        )\n",
                "\n",
                "        try:\n",
                "            completion = self.client.chat.completions.create(\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": system_prompt},\n",
                "                    {\"role\": \"user\", \"content\": f\"The user just said: {transcript}\"}\n",
                "                ],\n",
                "                model=settings.WINGMAN_MODEL, # <-- Using the FAST model\n",
                "                temperature=0.6,\n",
                "                max_tokens=60\n",
                "            )\n",
                "            advice = completion.choices[0].message.content.strip()\n",
                "            return advice\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Brain Service Error getting wingman advice: {e}\")\n",
                "            return \"WAITING\"\n",
                "\n",
                "    def extract_knowledge(self, transcript: str) -> List[dict]:\n",
                "        \"\"\"Extracts facts for the knowledge graph.\"\"\"\n",
                "        prompt = \"Extract relationships from the text. Return JSON ONLY: {'relationships': [{'source': 'A', 'target': 'B', 'relation': 'C'}]}. The entities must be clear.\"\n",
                "        try:\n",
                "            completion = self.client.chat.completions.create(\n",
                "                messages=[{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": transcript}],\n",
                "                model=settings.WINGMAN_MODEL, # Can use 8B for fast extraction\n",
                "                response_format={\"type\": \"json_object\"}\n",
                "            )\n",
                "            # Ensure safe JSON parsing\n",
                "            content = completion.choices[0].message.content\n",
                "            relationships = json.loads(content).get(\"relationships\", [])\n",
                "            return [r for r in relationships if r.get('source') and r.get('target')] # Filter malformed\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Brain Service Error extracting knowledge: {e}\")\n",
                "            return []\n",
                "\n",
                "    def ask_consultant(self, user_id: str, question: str, history: str, graph_context: str, vector_context: str) -> str:\n",
                "        \"\"\"Uses the powerful 70B model for detailed, context-aware answers.\"\"\"\n",
                "        system_prompt = (\n",
                "            \"You are an expert consultant AI named Bubbles. Your goal is to answer the user's detailed question \"\n",
                "            \"based on all available context: history, graph facts, and long-term memories.\"\n",
                "            \"\\n\\nRULES:\"\n",
                "            \"\\n1. **Do not** mention 'vectors', 'graphs', or 'context'. Simply use the information naturally.\"\n",
                "            \"\\n2. Provide a complete, short, and realistic answer.\"\n",
                "\n",
                "            f\"\\n\\n--- CONTEXT FOR BUBBLES ---\"\n",
                "            f\"\\nCONSULTANT HISTORY:\\n{history}\"\n",
                "            f\"\\nGRAPH FACTS:\\n{graph_context}\"\n",
                "            f\"\\nVEC MEMORIES:\\n{vector_context}\"\n",
                "            f\"\\n---------------------------\"\n",
                "        )\n",
                "        try:\n",
                "            completion = self.client.chat.completions.create(\n",
                "                messages=[\n",
                "                    {\"role\": \"system\", \"content\": system_prompt},\n",
                "                    {\"role\": \"user\", \"content\": question}\n",
                "                ],\n",
                "                model=settings.CONSULTANT_MODEL, # <-- Using the POWERFUL model\n",
                "                temperature=0.7,\n",
                "                max_tokens=400\n",
                "            )\n",
                "            answer = completion.choices[0].message.content\n",
                "            return answer\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ Brain Service Error asking consultant: {e}\")\n",
                "            return \"I'm having trouble thinking right now, please try again in a moment. - Bubbles\"\n",
                "\n",
                "# Initialize Services\n",
                "graph_svc = GraphService()\n",
                "vector_svc = VectorService()\n",
                "brain_svc = BrainService()\n",
                "session_svc = SessionService()\n",
                "\n",
                "\n",
                "# --- 3B. FastAPI Server (Token Generation & Consultant) ---\n",
                "\n",
                "app = FastAPI()\n",
                "app.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\n",
                "\n",
                "@app.get(\"/\")\n",
                "def root():\n",
                "    return {\"status\": \"Bubbles Brain Online\", \"consultant_model\": settings.CONSULTANT_MODEL, \"wingman_model\": settings.WINGMAN_MODEL}\n",
                "\n",
                "@app.get(\"/getToken\")\n",
                "async def get_token(userId: str, roomName: str = \"default-room\"):\n",
                "    \"\"\"Generates a LiveKit JWT token for a user to join a room.\"\"\"\n",
                "    token = api.AccessToken(settings.LIVEKIT_API_KEY, settings.LIVEKIT_API_SECRET)\n",
                "    token.with_identity(userId)\n",
                "    token.with_name(userId)\n",
                "    token.with_grants(api.VideoGrants(\n",
                "        room_join=True, room=roomName, can_publish=True, can_subscribe=True\n",
                "    ))\n",
                "    jwt_token = token.to_jwt()\n",
                "    return {\"token\": jwt_token, \"url\": settings.LIVEKIT_URL}\n",
                "\n",
                "class ConsultantRequest(BaseModel):\n",
                "    user_id: str # This should be the UUID used in Supabase\n",
                "    question: str\n",
                "\n",
                "@app.post(\"/ask_consultant\")\n",
                "async def ask_consultant_endpoint(req: ConsultantRequest):\n",
                "    \"\"\"Handles the detailed, asynchronous consultant queries (using 70B model).\"\"\"\n",
                "\n",
                "    # 1. Fetch Contexts\n",
                "    # Note: GraphService.load_graph() is called here to ensure graph is in RAM before finding context\n",
                "    graph_svc.load_graph(req.user_id)\n",
                "    g_ctx = graph_svc.find_context(req.user_id, req.question, top_k=10)\n",
                "    v_ctx = vector_svc.search_memory(req.user_id, req.question)\n",
                "    h_ctx = session_svc.fetch_consultant_history(req.user_id, limit=5)\n",
                "\n",
                "    # 2. Get Answer from Powerful LLM\n",
                "    answer = brain_svc.ask_consultant(req.user_id, req.question, h_ctx, g_ctx, v_ctx)\n",
                "\n",
                "    # 3. Log the Q&A pair\n",
                "    session_svc.log_consultant_qa(req.user_id, req.question, answer)\n",
                "\n",
                "    # 4. Save and remove graph from memory\n",
                "    graph_svc.save_graph(req.user_id)\n",
                "\n",
                "    return {\"answer\": answer}\n",
                "\n",
                "class WingmanRequest(BaseModel):\n",
                "    user_id: str\n",
                "    transcript: str\n",
                "\n",
                "@app.post(\"/process_transcript_wingman\")\n",
                "async def process_transcript_wingman(req: WingmanRequest):\n",
                "    \"\"\"\n",
                "    Receives a transcript (from 'Other') directly from the client.\n",
                "    Returns immediate advice (Wingman mode).\n",
                "    OPTIMIZED: Does NOT update graphs/vectors here. Only reads.\n",
                "    \"\"\"\n",
                "    user_id = req.user_id\n",
                "    transcript = req.transcript\n",
                "\n",
                "    print(f\"ðŸ“¨ Wingman Request from {user_id}: {transcript}\")\n",
                "\n",
                "    # 1. Load Contexts (READ ONLY)\n",
                "    graph_svc.load_graph(user_id)\n",
                "    g_ctx = graph_svc.find_context(user_id, transcript)\n",
                "    v_ctx = vector_svc.search_memory(user_id, transcript)\n",
                "\n",
                "    # 2. Get Advice\n",
                "    advice = brain_svc.get_wingman_advice(user_id, transcript, g_ctx, v_ctx)\n",
                "\n",
                "    return {\"advice\": advice}\n",
                "\n",
                "class SaveSessionRequest(BaseModel):\n",
                "    user_id: str\n",
                "    transcript: str\n",
                "    logs: List[Dict[str, Any]]\n",
                "\n",
                "@app.post(\"/save_session\")\n",
                "async def save_session_endpoint(req: SaveSessionRequest):\n",
                "    \"\"\"\n",
                "    Called when session ends.\n",
                "    1. Creates Session Record in DB.\n",
                "    2. Logs all messages to DB.\n",
                "    3. Updates Vector Memory.\n",
                "    4. Updates Knowledge Graph.\n",
                "    \"\"\"\n",
                "    user_id = req.user_id\n",
                "    transcript = req.transcript\n",
                "    logs = req.logs\n",
                "    \n",
                "    print(f\"ðŸ’¾ Saving Session for {user_id}...\")\n",
                "\n",
                "    # 1. Create Session in DB\n",
                "    session_id = session_svc.create_session_record(user_id, title=f\"Session {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
                "\n",
                "    # 2. Log Messages\n",
                "    session_svc.log_batch_messages(session_id, logs)\n",
                "\n",
                "    # 3. Save to Vector Memory\n",
                "    await vector_svc.save_memory(user_id, f\"Session Transcript: {transcript}\")\n",
                "\n",
                "    # 4. Extract Knowledge Graph\n",
                "    new_rels = brain_svc.extract_knowledge(transcript)\n",
                "    if new_rels:\n",
                "        graph_svc.load_graph(user_id) # Ensure loaded\n",
                "        graph_svc.update_local_graph(user_id, new_rels)\n",
                "        graph_svc.save_graph(user_id)\n",
                "\n",
                "    return {\"status\": \"success\", \"session_id\": session_id}\n",
                "\n",
                "\n",
                "# --- 3C. LiveKit Agent Worker (Wingman Mode) ---\n",
                "# NOTE: This is kept for reference but might be unused if we switch to client-side Deepgram.\n",
                "\n",
                "async def entrypoint(ctx: JobContext):\n",
                "    user_id = ctx.room.name # Assuming room name is the user_id for simplicity in this demo\n",
                "    print(f'ðŸ”´ AGENT CONNECTED to Room/User: {user_id}')\n",
                "\n",
                "    # Initialize Session and Graph\n",
                "    session_id = session_svc.start_session(user_id)\n",
                "    LIVE_SESSIONS[user_id] = session_id\n",
                "    graph_svc.load_graph(user_id)\n",
                "\n",
                "    # Configure Deepgram with Diarization\n",
                "    stt = deepgram.STT(\n",
                "        api_key=settings.DEEPGRAM_KEY,\n",
                "        model=\"nova-2\",\n",
                "        language=\"en-US\",\n",
                "        smart_format=True,\n",
                "        diarize=True,\n",
                "    )\n",
                "    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)\n",
                "\n",
                "    @ctx.room.on('track_subscribed')\n",
                "    def on_track_subscribed(track, publication, participant):\n",
                "        if track.kind == 'audio' and participant.identity == user_id:\n",
                "            print(f\"\\nðŸ”Š Agent: Subscribed to audio track from {user_id}.\")\n",
                "            stream = stt.stream()\n",
                "\n",
                "            async def process_transcripts():\n",
                "                async for event in stream:\n",
                "                    if event.type == deepgram.STTEventType.FINAL_TRANSCRIPT:\n",
                "                        transcript = event.alternatives[0].text\n",
                "                        if not transcript.strip(): continue\n",
                "\n",
                "                        # 1. Extract Speaker & Log\n",
                "                        # Note: Deepgram diarization returns speaker info in words. \n",
                "                        # We assume the first word's speaker represents the segment.\n",
                "                        speaker_id = 0\n",
                "                        if hasattr(event.alternatives[0], 'words') and event.alternatives[0].words:\n",
                "                            speaker_id = event.alternatives[0].words[0].speaker\n",
                "                        \n",
                "                        speaker_role = \"user\" if speaker_id == 0 else \"other\"\n",
                "                        \n",
                "                        session_svc.log_message(LIVE_SESSIONS[user_id], speaker_role, transcript)\n",
                "                        print(f\"ðŸ—£ï¸ {user_id} [Speaker {speaker_id}]: {transcript}\")\n",
                "                        # Send Transcript to Client\n",
                "                        t_payload = json.dumps({'type': 'transcript', 'text': transcript, 'speaker': speaker_role, 'is_final': True})\n",
                "                        await ctx.room.local_participant.publish_data(t_payload, reliable=True)\n",
                "\n",
                "                        # 2. Logic: Only generate advice for 'Others' (The person the user is talking to)\n",
                "                        if speaker_role == \"other\":\n",
                "                            # Gather Contexts (Graph, Vector)\n",
                "                            g_ctx = graph_svc.find_context(user_id, transcript)\n",
                "                            v_ctx = vector_svc.search_memory(user_id, transcript)\n",
                "\n",
                "                            # Get Real-time Advice (Fast 8B Model)\n",
                "                            # We pass the transcript of the 'other' person so the Wingman can suggest what to say/do.\n",
                "                            advice = brain_svc.get_wingman_advice(user_id, transcript, g_ctx, v_ctx)\n",
                "\n",
                "                            if advice != \"WAITING\":\n",
                "                                # Send Advice to Client\n",
                "                                print(f\"ðŸ’¡ Wingman Suggestion: {advice}\")\n",
                "                                payload = json.dumps({'type': 'assistant_response', 'text': advice, 'timestamp': datetime.now().isoformat()})\n",
                "                                await ctx.room.local_participant.publish_data(payload, reliable=True)\n",
                "\n",
                "                                # Log Agent Response\n",
                "                                session_svc.log_message(LIVE_SESSIONS[user_id], 'agent', advice)\n",
                "\n",
                "                        # 3. Extract Knowledge & Update In-Memory Graph (From BOTH speakers)\n",
                "                        new_rels = brain_svc.extract_knowledge(transcript)\n",
                "                        if new_rels:\n",
                "                            graph_svc.update_local_graph(user_id, new_rels)\n",
                "\n",
                "                        # 4. Save Transcript to Vector Memory\n",
                "                        await vector_svc.save_memory(user_id, f\"Speaker {speaker_id}: {transcript}\")\n",
                "\n",
                "            async def pump_audio():\n",
                "                async for frame in track.stream():\n",
                "                    stream.push_frame(frame)\n",
                "                await stream.aclose()\n",
                "\n",
                "            asyncio.create_task(process_transcripts())\n",
                "            asyncio.create_task(pump_audio())\n",
                "\n",
                "    @ctx.room.on('participant_disconnected')\n",
                "    def on_participant_disconnected(participant):\n",
                "        if participant.identity == user_id:\n",
                "            print(f\"\\nðŸ‘‹ Agent: User Left: {user_id}\")\n",
                "\n",
                "            # Final Persistence Steps\n",
                "            graph_svc.save_graph(user_id) # Save in-memory graph to DB\n",
                "            \n",
                "            # Clean up session state\n",
                "            if user_id in LIVE_SESSIONS:\n",
                "                del LIVE_SESSIONS[user_id]\n",
                "                print(f\"âœ… Session and Graph data cleared for {user_id}.\")\n",
                "\n",
                "\n",
                "# --- 3D. Main Execution (Server + Worker + Ngrok) ---\n",
                "\n",
                "async def main():\n",
                "    print(\"ðŸš€ Main: Starting Bubbles Brain Backend...\")\n",
                "\n",
                "    # Setup Ngrok\n",
                "    ngrok.set_auth_token(settings.NGROK_TOKEN)\n",
                "    for t in ngrok.get_tunnels():\n",
                "        try:\n",
                "            ngrok.disconnect(t.public_url)\n",
                "        except: pass\n",
                "\n",
                "    public_url = ngrok.connect(settings.PORT).public_url\n",
                "    print(f\"\\nðŸš€ BUBBLES BACKEND LIVE\")\n",
                "    print(f\"ðŸ”— Base URL: {public_url}\")\n",
                "    print(f\"ðŸ”‘ Token Endpoint: {public_url}/getToken?userId=UUID_FROM_AUTH_USERS\")\n",
                "    print(f\"ðŸ§  Consultant Model: {settings.CONSULTANT_MODEL} | âš¡ Wingman Model: {settings.WINGMAN_MODEL}\")\n",
                "\n",
                "    # QR Code for easy client setup\n",
                "    qr = qrcode.QRCode(box_size=5, border=2)\n",
                "    qr.add_data(public_url)\n",
                "    qr.make(fit=True)\n",
                "    try:\n",
                "        from IPython.display import display\n",
                "        print(\"\\nðŸ‘‡ SCAN THIS IN FLUTTER APP ðŸ‘‡\")\n",
                "        display(qr.make_image(fill_color=\"black\", back_color=\"white\"))\n",
                "    except: pass\n",
                "\n",
                "    # ADAPTER: Define a request_fnc for the LiveKit Worker\n",
                "    async def request_fnc(job_request: JobRequest):\n",
                "        # IMPORTANT: Use job_request.participant_identity as the user_id for consistency\n",
                "        print(f\"âš¡ Worker: Accepting Job for User: {job_request.participant_identity} in Room: {job_request.room_name}\")\n",
                "        await job_request.accept(entrypoint)\n",
                "\n",
                "    opts = WorkerOptions(\n",
                "        request_fnc=request_fnc,\n",
                "        ws_url=settings.LIVEKIT_URL,\n",
                "        api_key=settings.LIVEKIT_API_KEY,\n",
                "        api_secret=settings.LIVEKIT_API_SECRET\n",
                "    )\n",
                "    worker = Worker(opts)\n",
                "\n",
                "    config = uvicorn.Config(app, host=settings.HOST, port=settings.PORT, log_level=\"error\")\n",
                "    server = uvicorn.Server(config)\n",
                "\n",
                "    print(\"â–¶ï¸ Main: Running server and worker concurrently...\")\n",
                "    # We run both, but the client might not connect to LiveKit anymore.\n",
                "    await asyncio.gather(server.serve(), worker.run())\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    try:\n",
                "        loop = asyncio.get_event_loop()\n",
                "        loop.run_until_complete(main())\n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nShutting down...\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}